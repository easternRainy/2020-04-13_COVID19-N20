{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use RNN to model COVID-19 Culmulative Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from utils import *\n",
    "from utils.paths import *\n",
    "import utils.paths as p\n",
    "from utils.plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_paths(\"DateWorld\")\n",
    "countries, index_country, country_index = get_countries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{p.DRP}/{index_country[173]}.csv\")\n",
    "df = df.drop(['country', 'date', 'tagged_day'], axis=1)\n",
    "df.iloc[0]['confirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series(df, window):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(df)-window):\n",
    "        X.append(df.iloc[i:i+window].values)\n",
    "        y.append(df.iloc[i+window]['confirmed'])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = time_series(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1500000\n",
    "X = X/factor\n",
    "y = y/factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep1 = (int)(len(X) * 0.6)\n",
    "sep2 = (int)(len(X) * 0.8)\n",
    "X_train, y_train = X[:sep1], y[:sep1]\n",
    "X_test, y_test = X[sep1:sep2], y[sep1:sep2]\n",
    "X_pred, y_pred = X[sep2:], y[sep2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data_loader(X, y, batch_size):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidRnn(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers, output_size):\n",
    "        super(CovidRnn, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "             \n",
    "        self.rnn = nn.RNN(feature_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "        rnn_out = rnn_out.contiguous().view(-1,self.hidden_dim)\n",
    "        out = self.fc(rnn_out)\n",
    "        out = self.activation(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.randn(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "??nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim, hidden_dim, num_lstm_layers, output_size = 4, 4, 1, 1\n",
    "model = CovidRnn(feature_dim, hidden_dim, num_lstm_layers, output_size)\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.00000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "train_data_torch = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data_torch = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "pred_data_torch = TensorDataset(torch.from_numpy(X_pred), torch.from_numpy(y_pred))\n",
    "\n",
    "train_loader = DataLoader(train_data_torch, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data_torch, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "pred_loader = DataLoader(pred_data_torch, shuffle=False, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "for ix, iy in train_loader:\n",
    "    print(ix[0].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Step: 1 Loss: 0.004009926225990057 Val Loss: 0.00853828527033329\n",
      "Saving 0\n",
      "Epoch: 1/100 Step: 2 Loss: 0.0018768971785902977 Val Loss: 0.003631303319707513\n",
      "Saving 1\n",
      "Epoch: 1/100 Step: 3 Loss: 0.0043858750723302364 Val Loss: 0.0070287613198161125\n",
      "Epoch: 1/100 Step: 4 Loss: 0.020195428282022476 Val Loss: 0.028872227296233177\n",
      "Epoch: 1/100 Step: 5 Loss: 0.07649634778499603 Val Loss: 0.07356136292219162\n",
      "Epoch: 2/100 Step: 6 Loss: 0.0015409215120598674 Val Loss: 0.007288489956408739\n",
      "Epoch: 2/100 Step: 7 Loss: 0.0018757299985736609 Val Loss: 0.00403704633936286\n",
      "Epoch: 2/100 Step: 8 Loss: 0.0043856599368155 Val Loss: 0.012537849135696888\n",
      "Epoch: 2/100 Step: 9 Loss: 0.020194973796606064 Val Loss: 0.033386413007974625\n",
      "Epoch: 2/100 Step: 10 Loss: 0.0764954537153244 Val Loss: 0.07320892065763474\n",
      "Epoch: 3/100 Step: 11 Loss: 0.0019283536821603775 Val Loss: 0.006698396522551775\n",
      "Epoch: 3/100 Step: 12 Loss: 0.0018747898284345865 Val Loss: 0.013671246357262135\n",
      "Epoch: 3/100 Step: 13 Loss: 0.004385445732623339 Val Loss: 0.008007329888641834\n",
      "Epoch: 3/100 Step: 14 Loss: 0.0201945248991251 Val Loss: 0.02202608808875084\n",
      "Epoch: 3/100 Step: 15 Loss: 0.07649455219507217 Val Loss: 0.07574958354234695\n",
      "Saving 2\n",
      "Epoch: 4/100 Step: 16 Loss: 0.009451618418097496 Val Loss: 0.00424920255318284\n",
      "Saving 3\n",
      "Epoch: 4/100 Step: 17 Loss: 0.0018713787430897355 Val Loss: 0.011319572106003761\n",
      "Epoch: 4/100 Step: 18 Loss: 0.004385234322398901 Val Loss: 0.007178017403930426\n",
      "Epoch: 4/100 Step: 19 Loss: 0.02019408345222473 Val Loss: 0.028962954878807068\n",
      "Epoch: 4/100 Step: 20 Loss: 0.07649368792772293 Val Loss: 0.08922779560089111\n",
      "Epoch: 5/100 Step: 21 Loss: 0.0013541391817852855 Val Loss: 0.003234278876334429\n",
      "Saving 4\n",
      "Epoch: 5/100 Step: 22 Loss: 0.0018755890196189284 Val Loss: 0.006278481334447861\n",
      "Epoch: 5/100 Step: 23 Loss: 0.004385034553706646 Val Loss: 0.015493661165237427\n",
      "Epoch: 5/100 Step: 24 Loss: 0.020193645730614662 Val Loss: 0.03286176174879074\n",
      "Epoch: 5/100 Step: 25 Loss: 0.07649282366037369 Val Loss: 0.09525399655103683\n",
      "Epoch: 6/100 Step: 26 Loss: 0.002206065459176898 Val Loss: 0.014094927348196507\n",
      "Epoch: 6/100 Step: 27 Loss: 0.0018757365178316832 Val Loss: 0.005460384301841259\n",
      "Epoch: 6/100 Step: 28 Loss: 0.004384830128401518 Val Loss: 0.011641406454145908\n",
      "Epoch: 6/100 Step: 29 Loss: 0.020193198695778847 Val Loss: 0.027655133977532387\n",
      "Epoch: 6/100 Step: 30 Loss: 0.07649195194244385 Val Loss: 0.0767090693116188\n",
      "Saving 5\n",
      "Epoch: 7/100 Step: 31 Loss: 0.0017738083843141794 Val Loss: 0.00521824648603797\n",
      "Saving 6\n",
      "Epoch: 7/100 Step: 32 Loss: 0.0018736987840384245 Val Loss: 0.00864998809993267\n",
      "Epoch: 7/100 Step: 33 Loss: 0.004384621046483517 Val Loss: 0.0083194924518466\n",
      "Epoch: 7/100 Step: 34 Loss: 0.020192762836813927 Val Loss: 0.031972408294677734\n",
      "Epoch: 7/100 Step: 35 Loss: 0.07649107277393341 Val Loss: 0.08340500295162201\n",
      "Epoch: 8/100 Step: 36 Loss: 0.0012423902517184615 Val Loss: 0.004024392459541559\n",
      "Saving 7\n",
      "Epoch: 8/100 Step: 37 Loss: 0.001876331982202828 Val Loss: 0.006733859423547983\n",
      "Epoch: 8/100 Step: 38 Loss: 0.0043844198808074 Val Loss: 0.010935545898973942\n",
      "Epoch: 8/100 Step: 39 Loss: 0.020192323252558708 Val Loss: 0.028839608654379845\n",
      "Epoch: 8/100 Step: 40 Loss: 0.07649020850658417 Val Loss: 0.0795203447341919\n",
      "Epoch: 9/100 Step: 41 Loss: 0.0027698189951479435 Val Loss: 0.004778759088367224\n",
      "Saving 8\n",
      "Epoch: 9/100 Step: 42 Loss: 0.001873835688456893 Val Loss: 0.0033405644353479147\n",
      "Saving 9\n",
      "Epoch: 9/100 Step: 43 Loss: 0.004384211264550686 Val Loss: 0.012747217901051044\n",
      "Epoch: 9/100 Step: 44 Loss: 0.020191887393593788 Val Loss: 0.024304604157805443\n",
      "Epoch: 9/100 Step: 45 Loss: 0.07648934423923492 Val Loss: 0.0783643051981926\n",
      "Epoch: 10/100 Step: 46 Loss: 0.006080930121243 Val Loss: 0.007946132682263851\n",
      "Epoch: 10/100 Step: 47 Loss: 0.0018710697768256068 Val Loss: 0.0033265079837292433\n",
      "Saving 10\n",
      "Epoch: 10/100 Step: 48 Loss: 0.004384005907922983 Val Loss: 0.006814541760832071\n",
      "Epoch: 10/100 Step: 49 Loss: 0.020191451534628868 Val Loss: 0.0240237545222044\n",
      "Epoch: 10/100 Step: 50 Loss: 0.07648848742246628 Val Loss: 0.0870121568441391\n",
      "Epoch: 11/100 Step: 51 Loss: 0.00735828373581171 Val Loss: 0.004848395939916372\n",
      "Epoch: 11/100 Step: 52 Loss: 0.001876871450804174 Val Loss: 0.003850119886919856\n",
      "Epoch: 11/100 Step: 53 Loss: 0.004383810330182314 Val Loss: 0.009216594509780407\n",
      "Epoch: 11/100 Step: 54 Loss: 0.0201910138130188 Val Loss: 0.02726360782980919\n",
      "Epoch: 11/100 Step: 55 Loss: 0.07648761570453644 Val Loss: 0.08023792505264282\n",
      "Epoch: 12/100 Step: 56 Loss: 0.0016816516872495413 Val Loss: 0.004634819459170103\n",
      "Epoch: 12/100 Step: 57 Loss: 0.0018728041322901845 Val Loss: 0.0034843471366912127\n",
      "Epoch: 12/100 Step: 58 Loss: 0.004383596125990152 Val Loss: 0.007367963902652264\n",
      "Epoch: 12/100 Step: 59 Loss: 0.020190570503473282 Val Loss: 0.023746032267808914\n",
      "Epoch: 12/100 Step: 60 Loss: 0.0764867439866066 Val Loss: 0.07935930788516998\n",
      "Epoch: 13/100 Step: 61 Loss: 0.0023244477342814207 Val Loss: 0.003296097507700324\n",
      "Saving 11\n",
      "Epoch: 13/100 Step: 62 Loss: 0.0018765944987535477 Val Loss: 0.003967692144215107\n",
      "Epoch: 13/100 Step: 63 Loss: 0.00438339589163661 Val Loss: 0.02712308242917061\n",
      "Epoch: 13/100 Step: 64 Loss: 0.020190129056572914 Val Loss: 0.02460418827831745\n",
      "Epoch: 13/100 Step: 65 Loss: 0.07648585736751556 Val Loss: 0.07634750753641129\n",
      "Saving 12\n",
      "Epoch: 14/100 Step: 66 Loss: 0.005033343099057674 Val Loss: 0.005778592545539141\n",
      "Saving 13\n",
      "Epoch: 14/100 Step: 67 Loss: 0.0018755076453089714 Val Loss: 0.005779055412858725\n",
      "Epoch: 14/100 Step: 68 Loss: 0.004383184481412172 Val Loss: 0.011592199094593525\n",
      "Epoch: 14/100 Step: 69 Loss: 0.020189674571156502 Val Loss: 0.02622194215655327\n",
      "Epoch: 14/100 Step: 70 Loss: 0.07648496329784393 Val Loss: 0.07408497482538223\n",
      "Epoch: 15/100 Step: 71 Loss: 0.003970893565565348 Val Loss: 0.004388078581541777\n",
      "Saving 14\n",
      "Epoch: 15/100 Step: 72 Loss: 0.0018710693111643195 Val Loss: 0.00397250521928072\n",
      "Saving 15\n",
      "Epoch: 15/100 Step: 73 Loss: 0.004382967017591 Val Loss: 0.009901664219796658\n",
      "Epoch: 15/100 Step: 74 Loss: 0.020189229398965836 Val Loss: 0.027794960886240005\n",
      "Epoch: 15/100 Step: 75 Loss: 0.0764840841293335 Val Loss: 0.0736319050192833\n",
      "Epoch: 16/100 Step: 76 Loss: 0.0016738094855099916 Val Loss: 0.004115539137274027\n",
      "Epoch: 16/100 Step: 77 Loss: 0.0018718673381954432 Val Loss: 0.003958610352128744\n",
      "Saving 16\n",
      "Epoch: 16/100 Step: 78 Loss: 0.004382763523608446 Val Loss: 0.0094719547778368\n",
      "Epoch: 16/100 Step: 79 Loss: 0.020188791677355766 Val Loss: 0.0235202107578516\n",
      "Epoch: 16/100 Step: 80 Loss: 0.07648322731256485 Val Loss: 0.08341344445943832\n",
      "Epoch: 17/100 Step: 81 Loss: 0.001298960647545755 Val Loss: 0.0033835184294730425\n",
      "Saving 17\n",
      "Epoch: 17/100 Step: 82 Loss: 0.001873496570624411 Val Loss: 0.003829631954431534\n",
      "Saving 18\n",
      "Epoch: 17/100 Step: 83 Loss: 0.0043825628235936165 Val Loss: 0.006592425983399153\n",
      "Epoch: 17/100 Step: 84 Loss: 0.020188355818390846 Val Loss: 0.025051984935998917\n",
      "Epoch: 17/100 Step: 85 Loss: 0.0764823630452156 Val Loss: 0.09119813144207001\n",
      "Epoch: 18/100 Step: 86 Loss: 0.0014804323436692357 Val Loss: 0.00939887948334217\n",
      "Epoch: 18/100 Step: 87 Loss: 0.0018732448806986213 Val Loss: 0.004738236777484417\n",
      "Epoch: 18/100 Step: 88 Loss: 0.004382357466965914 Val Loss: 0.009394967928528786\n",
      "Epoch: 18/100 Step: 89 Loss: 0.02018791250884533 Val Loss: 0.024989008903503418\n",
      "Epoch: 18/100 Step: 90 Loss: 0.07648149877786636 Val Loss: 0.0839424803853035\n",
      "Epoch: 19/100 Step: 91 Loss: 0.00299656274728477 Val Loss: 0.004641257226467133\n",
      "Saving 19\n",
      "Epoch: 19/100 Step: 92 Loss: 0.00187627540435642 Val Loss: 0.0037370447535067797\n",
      "Saving 20\n",
      "Epoch: 19/100 Step: 93 Loss: 0.004382156301289797 Val Loss: 0.008751978166401386\n",
      "Epoch: 19/100 Step: 94 Loss: 0.02018747106194496 Val Loss: 0.02871675230562687\n",
      "Epoch: 19/100 Step: 95 Loss: 0.07648061215877533 Val Loss: 0.07845591008663177\n",
      "Epoch: 20/100 Step: 96 Loss: 0.012642382644116879 Val Loss: 0.0055703287944197655\n",
      "Epoch: 20/100 Step: 97 Loss: 0.00186620291788131 Val Loss: 0.004580318927764893\n",
      "Epoch: 20/100 Step: 98 Loss: 0.004381934180855751 Val Loss: 0.008923274464905262\n",
      "Epoch: 20/100 Step: 99 Loss: 0.020187029615044594 Val Loss: 0.025752898305654526\n",
      "Epoch: 20/100 Step: 100 Loss: 0.07647974044084549 Val Loss: 0.07663585990667343\n",
      "Saving 21\n",
      "Epoch: 21/100 Step: 101 Loss: 0.00561043294146657 Val Loss: 0.006299653090536594\n",
      "Saving 22\n",
      "Epoch: 21/100 Step: 102 Loss: 0.0018718551145866513 Val Loss: 0.011886104010045528\n",
      "Epoch: 21/100 Step: 103 Loss: 0.004381740465760231 Val Loss: 0.0071711488999426365\n",
      "Epoch: 21/100 Step: 104 Loss: 0.02018660120666027 Val Loss: 0.02630487084388733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100 Step: 105 Loss: 0.07647888362407684 Val Loss: 0.08082183450460434\n",
      "Epoch: 22/100 Step: 106 Loss: 0.0013684455770999193 Val Loss: 0.004183880984783173\n",
      "Saving 23\n",
      "Epoch: 22/100 Step: 107 Loss: 0.0018747021676972508 Val Loss: 0.004351456183940172\n",
      "Saving 24\n",
      "Epoch: 22/100 Step: 108 Loss: 0.004381540697067976 Val Loss: 0.010740499012172222\n",
      "Epoch: 22/100 Step: 109 Loss: 0.020186159759759903 Val Loss: 0.022541966289281845\n",
      "Saving 25\n",
      "Epoch: 22/100 Step: 110 Loss: 0.0764780044555664 Val Loss: 0.08546353876590729\n",
      "Epoch: 23/100 Step: 111 Loss: 0.0029976002406328917 Val Loss: 0.003993254154920578\n",
      "Saving 26\n",
      "Epoch: 23/100 Step: 112 Loss: 0.001874305191449821 Val Loss: 0.0038055831100791693\n",
      "Saving 27\n",
      "Epoch: 23/100 Step: 113 Loss: 0.004381336271762848 Val Loss: 0.025718869641423225\n",
      "Epoch: 23/100 Step: 114 Loss: 0.020185718312859535 Val Loss: 0.02647939696907997\n",
      "Epoch: 23/100 Step: 115 Loss: 0.07647713273763657 Val Loss: 0.07726182043552399\n",
      "Saving 28\n",
      "Epoch: 24/100 Step: 116 Loss: 0.016255740076303482 Val Loss: 0.00352415069937706\n",
      "Saving 29\n",
      "Epoch: 24/100 Step: 117 Loss: 0.0018654571613296866 Val Loss: 0.006227745208889246\n",
      "Epoch: 24/100 Step: 118 Loss: 0.004381117876619101 Val Loss: 0.008226636797189713\n",
      "Epoch: 24/100 Step: 119 Loss: 0.020185278728604317 Val Loss: 0.027538444846868515\n",
      "Epoch: 24/100 Step: 120 Loss: 0.07647627592086792 Val Loss: 0.07670172303915024\n",
      "Epoch: 25/100 Step: 121 Loss: 0.001807986875064671 Val Loss: 0.008525627665221691\n",
      "Epoch: 25/100 Step: 122 Loss: 0.0018752256873995066 Val Loss: 0.00573870912194252\n",
      "Epoch: 25/100 Step: 123 Loss: 0.004380929283797741 Val Loss: 0.013995150104165077\n",
      "Epoch: 25/100 Step: 124 Loss: 0.020184846594929695 Val Loss: 0.028388263657689095\n",
      "Epoch: 25/100 Step: 125 Loss: 0.07647540420293808 Val Loss: 0.0872117429971695\n",
      "Epoch: 26/100 Step: 126 Loss: 0.009088685736060143 Val Loss: 0.0033226399682462215\n",
      "Saving 30\n",
      "Epoch: 26/100 Step: 127 Loss: 0.0018685105023905635 Val Loss: 0.00393983768299222\n",
      "Epoch: 26/100 Step: 128 Loss: 0.004380715545266867 Val Loss: 0.0077209556475281715\n",
      "Epoch: 26/100 Step: 129 Loss: 0.020184408873319626 Val Loss: 0.034546710550785065\n",
      "Epoch: 26/100 Step: 130 Loss: 0.07647456228733063 Val Loss: 0.07754779607057571\n",
      "Epoch: 27/100 Step: 131 Loss: 0.007977028377354145 Val Loss: 0.01148808654397726\n",
      "Epoch: 27/100 Step: 132 Loss: 0.0018702445086091757 Val Loss: 0.008392086252570152\n",
      "Epoch: 27/100 Step: 133 Loss: 0.004380517639219761 Val Loss: 0.007697843946516514\n",
      "Epoch: 27/100 Step: 134 Loss: 0.020183978602290154 Val Loss: 0.030807357281446457\n",
      "Epoch: 27/100 Step: 135 Loss: 0.07647370547056198 Val Loss: 0.07994435727596283\n",
      "Epoch: 28/100 Step: 136 Loss: 0.0015093941474333405 Val Loss: 0.014105132780969143\n",
      "Epoch: 28/100 Step: 137 Loss: 0.0018716119229793549 Val Loss: 0.006723593920469284\n",
      "Epoch: 28/100 Step: 138 Loss: 0.004380317870527506 Val Loss: 0.009149940684437752\n",
      "Epoch: 28/100 Step: 139 Loss: 0.020183542743325233 Val Loss: 0.028871970251202583\n",
      "Epoch: 28/100 Step: 140 Loss: 0.07647284120321274 Val Loss: 0.0742807537317276\n",
      "Epoch: 29/100 Step: 141 Loss: 0.004651277791708708 Val Loss: 0.00416549202054739\n",
      "Epoch: 29/100 Step: 142 Loss: 0.0018717319471761584 Val Loss: 0.017370929941534996\n",
      "Epoch: 29/100 Step: 143 Loss: 0.004380115773528814 Val Loss: 0.00695433933287859\n",
      "Epoch: 29/100 Step: 144 Loss: 0.02018311247229576 Val Loss: 0.025995580479502678\n",
      "Epoch: 29/100 Step: 145 Loss: 0.07647199183702469 Val Loss: 0.07317795604467392\n",
      "Epoch: 30/100 Step: 146 Loss: 0.0030958878342062235 Val Loss: 0.016540080308914185\n",
      "Epoch: 30/100 Step: 147 Loss: 0.0018745006527751684 Val Loss: 0.016025323420763016\n",
      "Epoch: 30/100 Step: 148 Loss: 0.004379912745207548 Val Loss: 0.0075826323591172695\n",
      "Epoch: 30/100 Step: 149 Loss: 0.020182669162750244 Val Loss: 0.024807844310998917\n",
      "Epoch: 30/100 Step: 150 Loss: 0.07647111266851425 Val Loss: 0.07840229570865631\n",
      "Epoch: 31/100 Step: 151 Loss: 0.001654952415265143 Val Loss: 0.014529203064739704\n",
      "Epoch: 31/100 Step: 152 Loss: 0.0018705232068896294 Val Loss: 0.0097336545586586\n",
      "Epoch: 31/100 Step: 153 Loss: 0.004379700869321823 Val Loss: 0.011558132246136665\n",
      "Epoch: 31/100 Step: 154 Loss: 0.020182227715849876 Val Loss: 0.02747241035103798\n",
      "Epoch: 31/100 Step: 155 Loss: 0.07647024095058441 Val Loss: 0.08249306678771973\n",
      "Epoch: 32/100 Step: 156 Loss: 0.006740962620824575 Val Loss: 0.004990384913980961\n",
      "Epoch: 32/100 Step: 157 Loss: 0.0018743856344372034 Val Loss: 0.009734896011650562\n",
      "Epoch: 32/100 Step: 158 Loss: 0.0043795001693069935 Val Loss: 0.00714762881398201\n",
      "Epoch: 32/100 Step: 159 Loss: 0.02018178626894951 Val Loss: 0.030166205018758774\n",
      "Epoch: 32/100 Step: 160 Loss: 0.07646936178207397 Val Loss: 0.09479356557130814\n",
      "Epoch: 33/100 Step: 161 Loss: 0.0018127242801710963 Val Loss: 0.003115542232990265\n",
      "Saving 31\n",
      "Epoch: 33/100 Step: 162 Loss: 0.0018693836173042655 Val Loss: 0.010341966524720192\n",
      "Epoch: 33/100 Step: 163 Loss: 0.004379287362098694 Val Loss: 0.007414993830025196\n",
      "Epoch: 33/100 Step: 164 Loss: 0.02018134295940399 Val Loss: 0.024965938180685043\n",
      "Epoch: 33/100 Step: 165 Loss: 0.07646848261356354 Val Loss: 0.07204567641019821\n",
      "Epoch: 34/100 Step: 166 Loss: 0.0015928444918245077 Val Loss: 0.020442349836230278\n",
      "Epoch: 34/100 Step: 167 Loss: 0.001873209374025464 Val Loss: 0.005626089405268431\n",
      "Epoch: 34/100 Step: 168 Loss: 0.004379088059067726 Val Loss: 0.007586067076772451\n",
      "Epoch: 34/100 Step: 169 Loss: 0.020180899649858475 Val Loss: 0.03183523938059807\n",
      "Epoch: 34/100 Step: 170 Loss: 0.0764676108956337 Val Loss: 0.09142430871725082\n",
      "Epoch: 35/100 Step: 171 Loss: 0.006426627282053232 Val Loss: 0.006210654973983765\n",
      "Saving 32\n",
      "Epoch: 35/100 Step: 172 Loss: 0.0018744872650131583 Val Loss: 0.0038061237428337336\n",
      "Saving 33\n",
      "Epoch: 35/100 Step: 173 Loss: 0.00437887804582715 Val Loss: 0.007654779125005007\n",
      "Epoch: 35/100 Step: 174 Loss: 0.02018044702708721 Val Loss: 0.026134470477700233\n",
      "Epoch: 35/100 Step: 175 Loss: 0.07646671682596207 Val Loss: 0.09597824513912201\n",
      "Epoch: 36/100 Step: 176 Loss: 0.004277296829968691 Val Loss: 0.003171520074829459\n",
      "Saving 34\n",
      "Epoch: 36/100 Step: 177 Loss: 0.0018725964473560452 Val Loss: 0.0033069364726543427\n",
      "Epoch: 36/100 Step: 178 Loss: 0.004378663841634989 Val Loss: 0.009554390795528889\n",
      "Epoch: 36/100 Step: 179 Loss: 0.0201799888163805 Val Loss: 0.022158365696668625\n",
      "Epoch: 36/100 Step: 180 Loss: 0.07646580785512924 Val Loss: 0.07299401611089706\n",
      "Epoch: 37/100 Step: 181 Loss: 0.0023480760864913464 Val Loss: 0.014911036007106304\n",
      "Epoch: 37/100 Step: 182 Loss: 0.0018709719879552722 Val Loss: 0.007324868347495794\n",
      "Epoch: 37/100 Step: 183 Loss: 0.004378450568765402 Val Loss: 0.015759501606225967\n",
      "Epoch: 37/100 Step: 184 Loss: 0.020179545506834984 Val Loss: 0.026483898982405663\n",
      "Epoch: 37/100 Step: 185 Loss: 0.07646494358778 Val Loss: 0.07345635443925858\n",
      "Epoch: 38/100 Step: 186 Loss: 0.005465184338390827 Val Loss: 0.0044711134396493435\n",
      "Epoch: 38/100 Step: 187 Loss: 0.001872162683866918 Val Loss: 0.0040336488746106625\n",
      "Epoch: 38/100 Step: 188 Loss: 0.0043782456777989864 Val Loss: 0.006423225160688162\n",
      "Epoch: 38/100 Step: 189 Loss: 0.020179104059934616 Val Loss: 0.02807890810072422\n",
      "Epoch: 38/100 Step: 190 Loss: 0.07646405696868896 Val Loss: 0.07521118968725204\n",
      "Epoch: 39/100 Step: 191 Loss: 0.004088119603693485 Val Loss: 0.009829306043684483\n",
      "Epoch: 39/100 Step: 192 Loss: 0.0018685944378376007 Val Loss: 0.0033427930902689695\n",
      "Epoch: 39/100 Step: 193 Loss: 0.004378034267574549 Val Loss: 0.0077680316753685474\n",
      "Epoch: 39/100 Step: 194 Loss: 0.02017865888774395 Val Loss: 0.023859461769461632\n",
      "Epoch: 39/100 Step: 195 Loss: 0.07646318525075912 Val Loss: 0.08268804103136063\n",
      "Epoch: 40/100 Step: 196 Loss: 0.004217180889099836 Val Loss: 0.0046934583224356174\n",
      "Epoch: 40/100 Step: 197 Loss: 0.0018695200560614467 Val Loss: 0.003653263207525015\n",
      "Epoch: 40/100 Step: 198 Loss: 0.004377830773591995 Val Loss: 0.00968619342893362\n",
      "Epoch: 40/100 Step: 199 Loss: 0.020178215578198433 Val Loss: 0.0262347012758255\n",
      "Epoch: 40/100 Step: 200 Loss: 0.07646230608224869 Val Loss: 0.07642127573490143\n",
      "Epoch: 41/100 Step: 201 Loss: 0.004386357497423887 Val Loss: 0.003935066517442465\n",
      "Epoch: 41/100 Step: 202 Loss: 0.00186828407458961 Val Loss: 0.008152048103511333\n",
      "Epoch: 41/100 Step: 203 Loss: 0.004377624485641718 Val Loss: 0.013371939770877361\n",
      "Epoch: 41/100 Step: 204 Loss: 0.02017778530716896 Val Loss: 0.03625333681702614\n",
      "Epoch: 41/100 Step: 205 Loss: 0.07646145671606064 Val Loss: 0.08603416383266449\n",
      "Epoch: 42/100 Step: 206 Loss: 0.0015014728996902704 Val Loss: 0.007635383866727352\n",
      "Epoch: 42/100 Step: 207 Loss: 0.0018682049121707678 Val Loss: 0.008040225133299828\n",
      "Epoch: 42/100 Step: 208 Loss: 0.004377427045255899 Val Loss: 0.012205712497234344\n",
      "Epoch: 42/100 Step: 209 Loss: 0.020177364349365234 Val Loss: 0.029223378747701645\n",
      "Epoch: 42/100 Step: 210 Loss: 0.07646063715219498 Val Loss: 0.08421312272548676\n",
      "Epoch: 43/100 Step: 211 Loss: 0.007135644555091858 Val Loss: 0.007361737545579672\n",
      "Epoch: 43/100 Step: 212 Loss: 0.0018692957237362862 Val Loss: 0.004179741255939007\n",
      "Epoch: 43/100 Step: 213 Loss: 0.0043772319331765175 Val Loss: 0.009039659984409809\n",
      "Epoch: 43/100 Step: 214 Loss: 0.020176934078335762 Val Loss: 0.028161508962512016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/100 Step: 215 Loss: 0.07645978033542633 Val Loss: 0.08494091033935547\n",
      "Epoch: 44/100 Step: 216 Loss: 0.0021678495686501265 Val Loss: 0.010446249507367611\n",
      "Epoch: 44/100 Step: 217 Loss: 0.0018702175002545118 Val Loss: 0.0039368742145597935\n",
      "Epoch: 44/100 Step: 218 Loss: 0.004377031698822975 Val Loss: 0.014412691816687584\n",
      "Epoch: 44/100 Step: 219 Loss: 0.020176509395241737 Val Loss: 0.02665281668305397\n",
      "Epoch: 44/100 Step: 220 Loss: 0.07645893096923828 Val Loss: 0.07525273412466049\n",
      "Epoch: 45/100 Step: 221 Loss: 0.0022148783318698406 Val Loss: 0.01229052897542715\n",
      "Epoch: 45/100 Step: 222 Loss: 0.00187165557872504 Val Loss: 0.0033679683692753315\n",
      "Epoch: 45/100 Step: 223 Loss: 0.0043768309988081455 Val Loss: 0.007272628135979176\n",
      "Epoch: 45/100 Step: 224 Loss: 0.020176073536276817 Val Loss: 0.02323930524289608\n",
      "Epoch: 45/100 Step: 225 Loss: 0.07645805925130844 Val Loss: 0.08022834360599518\n",
      "Epoch: 46/100 Step: 226 Loss: 0.0022921375930309296 Val Loss: 0.0038184409495443106\n",
      "Epoch: 46/100 Step: 227 Loss: 0.001871350803412497 Val Loss: 0.004744391422718763\n",
      "Epoch: 46/100 Step: 228 Loss: 0.0043766251765191555 Val Loss: 0.007323018275201321\n",
      "Epoch: 46/100 Step: 229 Loss: 0.020175626501441002 Val Loss: 0.03435484692454338\n",
      "Epoch: 46/100 Step: 230 Loss: 0.0764571875333786 Val Loss: 0.08022163808345795\n",
      "Epoch: 47/100 Step: 231 Loss: 0.002535884967073798 Val Loss: 0.005381565075367689\n",
      "Epoch: 47/100 Step: 232 Loss: 0.0018726101843640208 Val Loss: 0.0033157963771373034\n",
      "Epoch: 47/100 Step: 233 Loss: 0.0043764193542301655 Val Loss: 0.006662983912974596\n",
      "Epoch: 47/100 Step: 234 Loss: 0.020175179466605186 Val Loss: 0.023356541991233826\n",
      "Epoch: 47/100 Step: 235 Loss: 0.07645630091428757 Val Loss: 0.07574193179607391\n",
      "Epoch: 48/100 Step: 236 Loss: 0.0030016449745744467 Val Loss: 0.005919317249208689\n",
      "Epoch: 48/100 Step: 237 Loss: 0.0018692649900913239 Val Loss: 0.01356525532901287\n",
      "Epoch: 48/100 Step: 238 Loss: 0.0043762060813605785 Val Loss: 0.015058168210089207\n",
      "Epoch: 48/100 Step: 239 Loss: 0.02017473615705967 Val Loss: 0.023311154916882515\n",
      "Epoch: 48/100 Step: 240 Loss: 0.07645542174577713 Val Loss: 0.08554017543792725\n",
      "Epoch: 49/100 Step: 241 Loss: 0.005008558742702007 Val Loss: 0.007666673976927996\n",
      "Epoch: 49/100 Step: 242 Loss: 0.0018674747552722692 Val Loss: 0.005422215908765793\n",
      "Epoch: 49/100 Step: 243 Loss: 0.00437599653378129 Val Loss: 0.02169879898428917\n",
      "Epoch: 49/100 Step: 244 Loss: 0.020174292847514153 Val Loss: 0.024105440825223923\n",
      "Epoch: 49/100 Step: 245 Loss: 0.0764545425772667 Val Loss: 0.07438325881958008\n",
      "Epoch: 50/100 Step: 246 Loss: 0.0031301381532102823 Val Loss: 0.010796522721648216\n",
      "Epoch: 50/100 Step: 247 Loss: 0.0018668657867237926 Val Loss: 0.0056135328486561775\n",
      "Epoch: 50/100 Step: 248 Loss: 0.004375791177153587 Val Loss: 0.010086649097502232\n",
      "Epoch: 50/100 Step: 249 Loss: 0.02017386071383953 Val Loss: 0.027903441339731216\n",
      "Epoch: 50/100 Step: 250 Loss: 0.07645370066165924 Val Loss: 0.08481265604496002\n",
      "Epoch: 51/100 Step: 251 Loss: 0.0015054421965032816 Val Loss: 0.007842333056032658\n",
      "Epoch: 51/100 Step: 252 Loss: 0.0018688557902351022 Val Loss: 0.0037403267342597246\n",
      "Epoch: 51/100 Step: 253 Loss: 0.00437559699639678 Val Loss: 0.010434661991894245\n",
      "Epoch: 51/100 Step: 254 Loss: 0.020173432305455208 Val Loss: 0.021542271599173546\n",
      "Epoch: 51/100 Step: 255 Loss: 0.07645285129547119 Val Loss: 0.08650244772434235\n",
      "Epoch: 52/100 Step: 256 Loss: 0.0010678760008886456 Val Loss: 0.003886882681399584\n",
      "Epoch: 52/100 Step: 257 Loss: 0.0018674744060263038 Val Loss: 0.01259425189346075\n",
      "Epoch: 52/100 Step: 258 Loss: 0.004375395365059376 Val Loss: 0.012015966698527336\n",
      "Epoch: 52/100 Step: 259 Loss: 0.020173002034425735 Val Loss: 0.02290489338338375\n",
      "Epoch: 52/100 Step: 260 Loss: 0.07645200192928314 Val Loss: 0.07647688686847687\n",
      "Epoch: 53/100 Step: 261 Loss: 0.005255518015474081 Val Loss: 0.005371068138629198\n",
      "Epoch: 53/100 Step: 262 Loss: 0.0018702811794355512 Val Loss: 0.010238492861390114\n",
      "Epoch: 53/100 Step: 263 Loss: 0.004375196062028408 Val Loss: 0.007505218032747507\n",
      "Epoch: 53/100 Step: 264 Loss: 0.020172564312815666 Val Loss: 0.028118712827563286\n",
      "Epoch: 53/100 Step: 265 Loss: 0.0764511451125145 Val Loss: 0.07245485484600067\n",
      "Epoch: 54/100 Step: 266 Loss: 0.002585297217592597 Val Loss: 0.0032431839499622583\n",
      "Epoch: 54/100 Step: 267 Loss: 0.001869202358648181 Val Loss: 0.004402580205351114\n",
      "Epoch: 54/100 Step: 268 Loss: 0.004374988377094269 Val Loss: 0.007893617264926434\n",
      "Epoch: 54/100 Step: 269 Loss: 0.020172130316495895 Val Loss: 0.025652779266238213\n",
      "Epoch: 54/100 Step: 270 Loss: 0.07645026594400406 Val Loss: 0.06966423988342285\n",
      "Epoch: 55/100 Step: 271 Loss: 0.00441372487694025 Val Loss: 0.0040831719525158405\n",
      "Epoch: 55/100 Step: 272 Loss: 0.0018672874430194497 Val Loss: 0.022106092423200607\n",
      "Epoch: 55/100 Step: 273 Loss: 0.004374781157821417 Val Loss: 0.013595727272331715\n",
      "Epoch: 55/100 Step: 274 Loss: 0.02017168514430523 Val Loss: 0.027480173856019974\n",
      "Epoch: 55/100 Step: 275 Loss: 0.07644939422607422 Val Loss: 0.08354027569293976\n",
      "Epoch: 56/100 Step: 276 Loss: 0.019110262393951416 Val Loss: 0.0037190124858170748\n",
      "Epoch: 56/100 Step: 277 Loss: 0.0018620381597429514 Val Loss: 0.00519213592633605\n",
      "Epoch: 56/100 Step: 278 Loss: 0.004374572541564703 Val Loss: 0.007239121478050947\n",
      "Epoch: 56/100 Step: 279 Loss: 0.020171256735920906 Val Loss: 0.025652164593338966\n",
      "Epoch: 56/100 Step: 280 Loss: 0.07644853740930557 Val Loss: 0.07919976860284805\n",
      "Epoch: 57/100 Step: 281 Loss: 0.008902734145522118 Val Loss: 0.01578960008919239\n",
      "Epoch: 57/100 Step: 282 Loss: 0.0018641119822859764 Val Loss: 0.0070344568230211735\n",
      "Epoch: 57/100 Step: 283 Loss: 0.0043743751011788845 Val Loss: 0.009848462417721748\n",
      "Epoch: 57/100 Step: 284 Loss: 0.020170826464891434 Val Loss: 0.02294529415667057\n",
      "Epoch: 57/100 Step: 285 Loss: 0.07644770294427872 Val Loss: 0.08323226124048233\n",
      "Epoch: 58/100 Step: 286 Loss: 0.006286985706537962 Val Loss: 0.003474408993497491\n",
      "Epoch: 58/100 Step: 287 Loss: 0.0018708644201979041 Val Loss: 0.00883594062179327\n",
      "Epoch: 58/100 Step: 288 Loss: 0.004374184645712376 Val Loss: 0.008649328723549843\n",
      "Epoch: 58/100 Step: 289 Loss: 0.02017039619386196 Val Loss: 0.025953367352485657\n",
      "Epoch: 58/100 Step: 290 Loss: 0.07644683122634888 Val Loss: 0.08688176423311234\n",
      "Epoch: 59/100 Step: 291 Loss: 0.0015265584224835038 Val Loss: 0.010379517450928688\n",
      "Epoch: 59/100 Step: 292 Loss: 0.0018686143448576331 Val Loss: 0.017631104215979576\n",
      "Epoch: 59/100 Step: 293 Loss: 0.0043739741668105125 Val Loss: 0.00906150508671999\n",
      "Epoch: 59/100 Step: 294 Loss: 0.020169952884316444 Val Loss: 0.0243925042450428\n",
      "Epoch: 59/100 Step: 295 Loss: 0.07644595205783844 Val Loss: 0.08805524557828903\n",
      "Epoch: 60/100 Step: 296 Loss: 0.0017402665689587593 Val Loss: 0.0027875627856701612\n",
      "Saving 35\n",
      "Epoch: 60/100 Step: 297 Loss: 0.001868714578449726 Val Loss: 0.006432130001485348\n",
      "Epoch: 60/100 Step: 298 Loss: 0.004373767413198948 Val Loss: 0.008690079674124718\n",
      "Epoch: 60/100 Step: 299 Loss: 0.020169509574770927 Val Loss: 0.0257760938256979\n",
      "Epoch: 60/100 Step: 300 Loss: 0.0764450877904892 Val Loss: 0.08346273005008698\n",
      "Epoch: 61/100 Step: 301 Loss: 0.009098336100578308 Val Loss: 0.008842922747135162\n",
      "Saving 36\n",
      "Epoch: 61/100 Step: 302 Loss: 0.0018643864896148443 Val Loss: 0.004171967506408691\n",
      "Saving 37\n",
      "Epoch: 61/100 Step: 303 Loss: 0.0043735564686357975 Val Loss: 0.009289228357374668\n",
      "Epoch: 61/100 Step: 304 Loss: 0.020169073715806007 Val Loss: 0.022656377404928207\n",
      "Epoch: 61/100 Step: 305 Loss: 0.07644421607255936 Val Loss: 0.08448269218206406\n",
      "Epoch: 62/100 Step: 306 Loss: 0.016336211934685707 Val Loss: 0.004104176536202431\n",
      "Saving 38\n",
      "Epoch: 62/100 Step: 307 Loss: 0.0018605142831802368 Val Loss: 0.0052930088713765144\n",
      "Epoch: 62/100 Step: 308 Loss: 0.004373349715024233 Val Loss: 0.011547653004527092\n",
      "Epoch: 62/100 Step: 309 Loss: 0.02016863413155079 Val Loss: 0.027055084705352783\n",
      "Epoch: 62/100 Step: 310 Loss: 0.07644336670637131 Val Loss: 0.08923906832933426\n",
      "Epoch: 63/100 Step: 311 Loss: 0.0037633536849170923 Val Loss: 0.0034869585651904345\n",
      "Saving 39\n",
      "Epoch: 63/100 Step: 312 Loss: 0.001864990801550448 Val Loss: 0.003523983061313629\n",
      "Epoch: 63/100 Step: 313 Loss: 0.0043731555342674255 Val Loss: 0.006400447804480791\n",
      "Epoch: 63/100 Step: 314 Loss: 0.020168205723166466 Val Loss: 0.023223966360092163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100 Step: 315 Loss: 0.07644250243902206 Val Loss: 0.08734195679426193\n",
      "Epoch: 64/100 Step: 316 Loss: 0.0018348239827901125 Val Loss: 0.015631604939699173\n",
      "Epoch: 64/100 Step: 317 Loss: 0.0018674259772524238 Val Loss: 0.003654616652056575\n",
      "Epoch: 64/100 Step: 318 Loss: 0.004372959025204182 Val Loss: 0.007085825782269239\n",
      "Epoch: 64/100 Step: 319 Loss: 0.02016778104007244 Val Loss: 0.028338314965367317\n",
      "Epoch: 64/100 Step: 320 Loss: 0.07644166052341461 Val Loss: 0.08020015060901642\n",
      "Epoch: 65/100 Step: 321 Loss: 0.0027980634476989508 Val Loss: 0.00704615144059062\n",
      "Epoch: 65/100 Step: 322 Loss: 0.001865689642727375 Val Loss: 0.0057136123068630695\n",
      "Epoch: 65/100 Step: 323 Loss: 0.0043727559968829155 Val Loss: 0.008602256886661053\n",
      "Epoch: 65/100 Step: 324 Loss: 0.02016735076904297 Val Loss: 0.0268305204808712\n",
      "Epoch: 65/100 Step: 325 Loss: 0.07644081860780716 Val Loss: 0.07410929352045059\n",
      "Epoch: 66/100 Step: 326 Loss: 0.0019609923474490643 Val Loss: 0.006104657892137766\n",
      "Epoch: 66/100 Step: 327 Loss: 0.0018675468163564801 Val Loss: 0.01806878112256527\n",
      "Epoch: 66/100 Step: 328 Loss: 0.004372561816126108 Val Loss: 0.008244846947491169\n",
      "Epoch: 66/100 Step: 329 Loss: 0.020166927948594093 Val Loss: 0.03317955136299133\n",
      "Epoch: 66/100 Step: 330 Loss: 0.07643997669219971 Val Loss: 0.07748349010944366\n",
      "Epoch: 67/100 Step: 331 Loss: 0.0029383425135165453 Val Loss: 0.004098080564290285\n",
      "Epoch: 67/100 Step: 332 Loss: 0.001864617457613349 Val Loss: 0.0058671957813203335\n",
      "Epoch: 67/100 Step: 333 Loss: 0.004372358787804842 Val Loss: 0.008456796407699585\n",
      "Epoch: 67/100 Step: 334 Loss: 0.02016649954020977 Val Loss: 0.0237140990793705\n",
      "Epoch: 67/100 Step: 335 Loss: 0.07643913477659225 Val Loss: 0.08261451870203018\n",
      "Epoch: 68/100 Step: 336 Loss: 0.0026171053759753704 Val Loss: 0.0051062521524727345\n",
      "Epoch: 68/100 Step: 337 Loss: 0.0018664664821699262 Val Loss: 0.006788620259612799\n",
      "Epoch: 68/100 Step: 338 Loss: 0.004372165072709322 Val Loss: 0.01424460206180811\n",
      "Epoch: 68/100 Step: 339 Loss: 0.020166078582406044 Val Loss: 0.02726033702492714\n",
      "Epoch: 68/100 Step: 340 Loss: 0.0764383003115654 Val Loss: 0.07814177125692368\n",
      "Epoch: 69/100 Step: 341 Loss: 0.0017277635633945465 Val Loss: 0.01859920285642147\n",
      "Epoch: 69/100 Step: 342 Loss: 0.0018664234085008502 Val Loss: 0.004353032913058996\n",
      "Epoch: 69/100 Step: 343 Loss: 0.004371967166662216 Val Loss: 0.01717071607708931\n",
      "Epoch: 69/100 Step: 344 Loss: 0.02016565389931202 Val Loss: 0.025625411421060562\n",
      "Epoch: 69/100 Step: 345 Loss: 0.07643746584653854 Val Loss: 0.07317682355642319\n",
      "Epoch: 70/100 Step: 346 Loss: 0.0038728569634258747 Val Loss: 0.0031347365584224463\n",
      "Saving 40\n",
      "Epoch: 70/100 Step: 347 Loss: 0.0018643318908289075 Val Loss: 0.007013797294348478\n",
      "Epoch: 70/100 Step: 348 Loss: 0.004371766000986099 Val Loss: 0.02173839509487152\n",
      "Epoch: 70/100 Step: 349 Loss: 0.020165234804153442 Val Loss: 0.023852018639445305\n",
      "Epoch: 70/100 Step: 350 Loss: 0.07643663138151169 Val Loss: 0.09034966677427292\n",
      "Epoch: 71/100 Step: 351 Loss: 0.0029591345228254795 Val Loss: 0.004963512998074293\n",
      "Epoch: 71/100 Step: 352 Loss: 0.001866810955107212 Val Loss: 0.008153681643307209\n",
      "Epoch: 71/100 Step: 353 Loss: 0.004371569491922855 Val Loss: 0.007824580185115337\n",
      "Epoch: 71/100 Step: 354 Loss: 0.020164798945188522 Val Loss: 0.02534298039972782\n",
      "Epoch: 71/100 Step: 355 Loss: 0.07643575966358185 Val Loss: 0.08176908642053604\n",
      "Epoch: 72/100 Step: 356 Loss: 0.003760972758755088 Val Loss: 0.003883959725499153\n",
      "Epoch: 72/100 Step: 357 Loss: 0.0018652251455932856 Val Loss: 0.009822508320212364\n",
      "Epoch: 72/100 Step: 358 Loss: 0.004371364135295153 Val Loss: 0.009745003655552864\n",
      "Epoch: 72/100 Step: 359 Loss: 0.020164359360933304 Val Loss: 0.029506785795092583\n",
      "Epoch: 72/100 Step: 360 Loss: 0.0764349028468132 Val Loss: 0.07611697912216187\n",
      "Epoch: 73/100 Step: 361 Loss: 0.004475249443203211 Val Loss: 0.0035688288044184446\n",
      "Epoch: 73/100 Step: 362 Loss: 0.0018675150349736214 Val Loss: 0.0042686741799116135\n",
      "Epoch: 73/100 Step: 363 Loss: 0.004371164832264185 Val Loss: 0.0065193818882107735\n",
      "Epoch: 73/100 Step: 364 Loss: 0.020163923501968384 Val Loss: 0.023988064378499985\n",
      "Epoch: 73/100 Step: 365 Loss: 0.07643403112888336 Val Loss: 0.07427849620580673\n",
      "Epoch: 74/100 Step: 366 Loss: 0.0021868867333978415 Val Loss: 0.00584463682025671\n",
      "Epoch: 74/100 Step: 367 Loss: 0.0018668553093448281 Val Loss: 0.005667286459356546\n",
      "Epoch: 74/100 Step: 368 Loss: 0.0043709552846848965 Val Loss: 0.011185246519744396\n",
      "Epoch: 74/100 Step: 369 Loss: 0.020163482055068016 Val Loss: 0.023908410221338272\n",
      "Epoch: 74/100 Step: 370 Loss: 0.07643314450979233 Val Loss: 0.07062512636184692\n",
      "Epoch: 75/100 Step: 371 Loss: 0.008509139530360699 Val Loss: 0.007615580689162016\n",
      "Epoch: 75/100 Step: 372 Loss: 0.001861930126324296 Val Loss: 0.003802767489105463\n",
      "Epoch: 75/100 Step: 373 Loss: 0.004370746202766895 Val Loss: 0.00821351446211338\n",
      "Epoch: 75/100 Step: 374 Loss: 0.020163046196103096 Val Loss: 0.024254368618130684\n",
      "Epoch: 75/100 Step: 375 Loss: 0.07643229514360428 Val Loss: 0.07250189036130905\n",
      "Epoch: 76/100 Step: 376 Loss: 0.007026734761893749 Val Loss: 0.0035235534887760878\n",
      "Epoch: 76/100 Step: 377 Loss: 0.001864899881184101 Val Loss: 0.006121833808720112\n",
      "Epoch: 76/100 Step: 378 Loss: 0.004370551090687513 Val Loss: 0.007983791641891003\n",
      "Epoch: 76/100 Step: 379 Loss: 0.020162615925073624 Val Loss: 0.022236043587327003\n",
      "Epoch: 76/100 Step: 380 Loss: 0.07643145322799683 Val Loss: 0.07256008684635162\n",
      "Epoch: 77/100 Step: 381 Loss: 0.0012324802810326219 Val Loss: 0.003514191834256053\n",
      "Epoch: 77/100 Step: 382 Loss: 0.0018644945230334997 Val Loss: 0.008183179423213005\n",
      "Epoch: 77/100 Step: 383 Loss: 0.004370349925011396 Val Loss: 0.00956784002482891\n",
      "Epoch: 77/100 Step: 384 Loss: 0.020162193104624748 Val Loss: 0.02818927727639675\n",
      "Epoch: 77/100 Step: 385 Loss: 0.07643061131238937 Val Loss: 0.08195241540670395\n",
      "Epoch: 78/100 Step: 386 Loss: 0.0037753914948552847 Val Loss: 0.006424452178180218\n",
      "Epoch: 78/100 Step: 387 Loss: 0.0018617272144183517 Val Loss: 0.004300860222429037\n",
      "Epoch: 78/100 Step: 388 Loss: 0.004370148293673992 Val Loss: 0.010737471282482147\n",
      "Epoch: 78/100 Step: 389 Loss: 0.020161766558885574 Val Loss: 0.022407161071896553\n",
      "Epoch: 78/100 Step: 390 Loss: 0.07642976939678192 Val Loss: 0.07284007966518402\n",
      "Epoch: 79/100 Step: 391 Loss: 0.004912578500807285 Val Loss: 0.004383902531117201\n",
      "Epoch: 79/100 Step: 392 Loss: 0.0018684545066207647 Val Loss: 0.003727126168087125\n",
      "Epoch: 79/100 Step: 393 Loss: 0.004369958303868771 Val Loss: 0.008296267129480839\n",
      "Epoch: 79/100 Step: 394 Loss: 0.020161336287856102 Val Loss: 0.025957975536584854\n",
      "Epoch: 79/100 Step: 395 Loss: 0.07642889767885208 Val Loss: 0.08130903542041779\n",
      "Epoch: 80/100 Step: 396 Loss: 0.006928774993866682 Val Loss: 0.005799928214401007\n",
      "Epoch: 80/100 Step: 397 Loss: 0.0018628217512741685 Val Loss: 0.006802600808441639\n",
      "Epoch: 80/100 Step: 398 Loss: 0.004369745496660471 Val Loss: 0.008578256703913212\n",
      "Epoch: 80/100 Step: 399 Loss: 0.020160896703600883 Val Loss: 0.02731703594326973\n",
      "Epoch: 80/100 Step: 400 Loss: 0.07642804086208344 Val Loss: 0.07773087918758392\n",
      "Epoch: 81/100 Step: 401 Loss: 0.005905858241021633 Val Loss: 0.006771013140678406\n",
      "Epoch: 81/100 Step: 402 Loss: 0.0018623012583702803 Val Loss: 0.0038038610946387053\n",
      "Epoch: 81/100 Step: 403 Loss: 0.004369537346065044 Val Loss: 0.007944529876112938\n",
      "Epoch: 81/100 Step: 404 Loss: 0.020160460844635963 Val Loss: 0.023443209007382393\n",
      "Epoch: 81/100 Step: 405 Loss: 0.07642717659473419 Val Loss: 0.07320540398359299\n",
      "Epoch: 82/100 Step: 406 Loss: 0.0026742133777588606 Val Loss: 0.013203631155192852\n",
      "Epoch: 82/100 Step: 407 Loss: 0.0018621160415932536 Val Loss: 0.008256428875029087\n",
      "Epoch: 82/100 Step: 408 Loss: 0.004369338508695364 Val Loss: 0.0075713214464485645\n",
      "Epoch: 82/100 Step: 409 Loss: 0.02016003243625164 Val Loss: 0.021967103704810143\n",
      "Epoch: 82/100 Step: 410 Loss: 0.07642632722854614 Val Loss: 0.07469400018453598\n",
      "Epoch: 83/100 Step: 411 Loss: 0.004779328592121601 Val Loss: 0.004795163869857788\n",
      "Epoch: 83/100 Step: 412 Loss: 0.0018661066424101591 Val Loss: 0.004996869713068008\n",
      "Epoch: 83/100 Step: 413 Loss: 0.004369142930954695 Val Loss: 0.007616186048835516\n",
      "Epoch: 83/100 Step: 414 Loss: 0.02015959657728672 Val Loss: 0.03137442097067833\n",
      "Epoch: 83/100 Step: 415 Loss: 0.0764254704117775 Val Loss: 0.07392819970846176\n",
      "Epoch: 84/100 Step: 416 Loss: 0.005854604300111532 Val Loss: 0.004467334598302841\n",
      "Epoch: 84/100 Step: 417 Loss: 0.0018639374757185578 Val Loss: 0.008176813833415508\n",
      "Epoch: 84/100 Step: 418 Loss: 0.004368940833956003 Val Loss: 0.009472130797803402\n",
      "Epoch: 84/100 Step: 419 Loss: 0.020159170031547546 Val Loss: 0.0323873907327652\n",
      "Epoch: 84/100 Step: 420 Loss: 0.07642461359500885 Val Loss: 0.07568398863077164\n",
      "Epoch: 85/100 Step: 421 Loss: 0.002293405355885625 Val Loss: 0.00731716537848115\n",
      "Epoch: 85/100 Step: 422 Loss: 0.0018670327262952924 Val Loss: 0.003919665236026049\n",
      "Epoch: 85/100 Step: 423 Loss: 0.004368741996586323 Val Loss: 0.009822162799537182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100 Step: 424 Loss: 0.020158732309937477 Val Loss: 0.02197016216814518\n",
      "Epoch: 85/100 Step: 425 Loss: 0.0764237493276596 Val Loss: 0.07621191442012787\n",
      "Epoch: 86/100 Step: 426 Loss: 0.004815057851374149 Val Loss: 0.0047562154941260815\n",
      "Epoch: 86/100 Step: 427 Loss: 0.0018632555147632957 Val Loss: 0.008323258720338345\n",
      "Epoch: 86/100 Step: 428 Loss: 0.004368531052023172 Val Loss: 0.007429937366396189\n",
      "Epoch: 86/100 Step: 429 Loss: 0.02015829272568226 Val Loss: 0.02121819369494915\n",
      "Epoch: 86/100 Step: 430 Loss: 0.07642287760972977 Val Loss: 0.08358119428157806\n",
      "Epoch: 87/100 Step: 431 Loss: 0.0060927048325538635 Val Loss: 0.005432221107184887\n",
      "Epoch: 87/100 Step: 432 Loss: 0.001864703604951501 Val Loss: 0.005137948784977198\n",
      "Epoch: 87/100 Step: 433 Loss: 0.004368327558040619 Val Loss: 0.007311173714697361\n",
      "Epoch: 87/100 Step: 434 Loss: 0.02015784941613674 Val Loss: 0.024067293852567673\n",
      "Epoch: 87/100 Step: 435 Loss: 0.07642201334238052 Val Loss: 0.08280978351831436\n",
      "Epoch: 88/100 Step: 436 Loss: 0.0035097578074783087 Val Loss: 0.008319885469973087\n",
      "Epoch: 88/100 Step: 437 Loss: 0.0018631855491548777 Val Loss: 0.005291360896080732\n",
      "Epoch: 88/100 Step: 438 Loss: 0.004368119407445192 Val Loss: 0.00854214932769537\n",
      "Epoch: 88/100 Step: 439 Loss: 0.020157411694526672 Val Loss: 0.023910675197839737\n",
      "Epoch: 88/100 Step: 440 Loss: 0.07642113417387009 Val Loss: 0.07968652248382568\n",
      "Epoch: 89/100 Step: 441 Loss: 0.00448279082775116 Val Loss: 0.0038794944994151592\n",
      "Epoch: 89/100 Step: 442 Loss: 0.0018659211928024888 Val Loss: 0.014710522256791592\n",
      "Epoch: 89/100 Step: 443 Loss: 0.00436791917309165 Val Loss: 0.00807006936520338\n",
      "Epoch: 89/100 Step: 444 Loss: 0.020156970247626305 Val Loss: 0.031805090606212616\n",
      "Epoch: 89/100 Step: 445 Loss: 0.07642025500535965 Val Loss: 0.08572281897068024\n",
      "Epoch: 90/100 Step: 446 Loss: 0.0025787746999412775 Val Loss: 0.008124197833240032\n",
      "Epoch: 90/100 Step: 447 Loss: 0.0018611534032970667 Val Loss: 0.0032327244989573956\n",
      "Epoch: 90/100 Step: 448 Loss: 0.0043677049688994884 Val Loss: 0.010122730396687984\n",
      "Epoch: 90/100 Step: 449 Loss: 0.020156534388661385 Val Loss: 0.02584676258265972\n",
      "Epoch: 90/100 Step: 450 Loss: 0.0764193907380104 Val Loss: 0.09378351271152496\n",
      "Epoch: 91/100 Step: 451 Loss: 0.0015327193541452289 Val Loss: 0.009417349472641945\n",
      "Epoch: 91/100 Step: 452 Loss: 0.001864133169874549 Val Loss: 0.004322270397096872\n",
      "Epoch: 91/100 Step: 453 Loss: 0.004367505665868521 Val Loss: 0.008906249888241291\n",
      "Epoch: 91/100 Step: 454 Loss: 0.02015608549118042 Val Loss: 0.027966227382421494\n",
      "Epoch: 91/100 Step: 455 Loss: 0.07641851902008057 Val Loss: 0.0762036070227623\n",
      "Epoch: 92/100 Step: 456 Loss: 0.002599473809823394 Val Loss: 0.007519069127738476\n",
      "Epoch: 92/100 Step: 457 Loss: 0.001865576021373272 Val Loss: 0.013130665756762028\n",
      "Epoch: 92/100 Step: 458 Loss: 0.00436730170622468 Val Loss: 0.007878114469349384\n",
      "Epoch: 92/100 Step: 459 Loss: 0.02015564776957035 Val Loss: 0.033020295202732086\n",
      "Epoch: 92/100 Step: 460 Loss: 0.07641764730215073 Val Loss: 0.08560393750667572\n",
      "Epoch: 93/100 Step: 461 Loss: 0.0010787126375362277 Val Loss: 0.006769825704395771\n",
      "Epoch: 93/100 Step: 462 Loss: 0.0018649896373972297 Val Loss: 0.011912742629647255\n",
      "Epoch: 93/100 Step: 463 Loss: 0.00436709588393569 Val Loss: 0.008049956522881985\n",
      "Epoch: 93/100 Step: 464 Loss: 0.020155208185315132 Val Loss: 0.029402580112218857\n",
      "Epoch: 93/100 Step: 465 Loss: 0.07641679048538208 Val Loss: 0.08198773860931396\n",
      "Epoch: 94/100 Step: 466 Loss: 0.006739090662449598 Val Loss: 0.0064795538783073425\n",
      "Epoch: 94/100 Step: 467 Loss: 0.0018587157828733325 Val Loss: 0.0036772957537323236\n",
      "Epoch: 94/100 Step: 468 Loss: 0.004366885405033827 Val Loss: 0.02270243875682354\n",
      "Epoch: 94/100 Step: 469 Loss: 0.02015477791428566 Val Loss: 0.022410836070775986\n",
      "Epoch: 94/100 Step: 470 Loss: 0.07641593366861343 Val Loss: 0.07384676486253738\n",
      "Epoch: 95/100 Step: 471 Loss: 0.022351861000061035 Val Loss: 0.009320219978690147\n",
      "Epoch: 95/100 Step: 472 Loss: 0.0018574055284261703 Val Loss: 0.007463403046131134\n",
      "Epoch: 95/100 Step: 473 Loss: 0.0043666851706802845 Val Loss: 0.010679683648049831\n",
      "Epoch: 95/100 Step: 474 Loss: 0.020154347643256187 Val Loss: 0.033729858696460724\n",
      "Epoch: 95/100 Step: 475 Loss: 0.07641508430242538 Val Loss: 0.07215926796197891\n",
      "Epoch: 96/100 Step: 476 Loss: 0.006945265922695398 Val Loss: 0.004092616494745016\n",
      "Epoch: 96/100 Step: 477 Loss: 0.0018596696900203824 Val Loss: 0.027542486786842346\n",
      "Epoch: 96/100 Step: 478 Loss: 0.004366488195955753 Val Loss: 0.00925331749022007\n",
      "Epoch: 96/100 Step: 479 Loss: 0.020153922960162163 Val Loss: 0.030952099710702896\n",
      "Epoch: 96/100 Step: 480 Loss: 0.07641423493623734 Val Loss: 0.08247905224561691\n",
      "Epoch: 97/100 Step: 481 Loss: 0.0031725873704999685 Val Loss: 0.005803283769637346\n",
      "Epoch: 97/100 Step: 482 Loss: 0.0018618537578731775 Val Loss: 0.007609005551785231\n",
      "Epoch: 97/100 Step: 483 Loss: 0.0043662916868925095 Val Loss: 0.007340333424508572\n",
      "Epoch: 97/100 Step: 484 Loss: 0.020153488963842392 Val Loss: 0.026842612773180008\n",
      "Epoch: 97/100 Step: 485 Loss: 0.07641338557004929 Val Loss: 0.07710160315036774\n",
      "Epoch: 98/100 Step: 486 Loss: 0.0051779416389763355 Val Loss: 0.004857812076807022\n",
      "Epoch: 98/100 Step: 487 Loss: 0.001865010941401124 Val Loss: 0.004321929533034563\n",
      "Epoch: 98/100 Step: 488 Loss: 0.004366092849522829 Val Loss: 0.009480210021138191\n",
      "Epoch: 98/100 Step: 489 Loss: 0.02015305869281292 Val Loss: 0.02388230338692665\n",
      "Epoch: 98/100 Step: 490 Loss: 0.07641250640153885 Val Loss: 0.08630795031785965\n",
      "Epoch: 99/100 Step: 491 Loss: 0.011586016975343227 Val Loss: 0.008882386609911919\n",
      "Epoch: 99/100 Step: 492 Loss: 0.0018566854996606708 Val Loss: 0.004622977692633867\n",
      "Epoch: 99/100 Step: 493 Loss: 0.004365877714008093 Val Loss: 0.006327726878225803\n",
      "Epoch: 99/100 Step: 494 Loss: 0.02015262469649315 Val Loss: 0.03334110975265503\n",
      "Epoch: 99/100 Step: 495 Loss: 0.0764116644859314 Val Loss: 0.07712491601705551\n",
      "Epoch: 100/100 Step: 496 Loss: 0.0016371719539165497 Val Loss: 0.0068257832899689674\n",
      "Epoch: 100/100 Step: 497 Loss: 0.001862817327491939 Val Loss: 0.013647958636283875\n",
      "Epoch: 100/100 Step: 498 Loss: 0.004365686327219009 Val Loss: 0.009909330867230892\n",
      "Epoch: 100/100 Step: 499 Loss: 0.020152190700173378 Val Loss: 0.030288202688097954\n",
      "Epoch: 100/100 Step: 500 Loss: 0.07641081511974335 Val Loss: 0.0803772360086441\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "counter = 0\n",
    "clip = 5\n",
    "print_every = 1\n",
    "\n",
    "min_loss = np.Inf\n",
    "min_diff = np.Inf\n",
    "seq_len = 10\n",
    "\n",
    "saving_index = 0\n",
    "\n",
    "model.train()\n",
    "h = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    #display(h.shape)\n",
    "    for X, y in train_loader:\n",
    "        counter += 1\n",
    "        X = X.reshape(seq_len,batch_size,feature_dim).float()\n",
    "        #display(X.shape)\n",
    "        # h = tuple([e.data for e in h])\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(X, h)\n",
    "        # display(output)\n",
    "        loss = criterion(output.squeeze(), y.float())\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            test_h = model.init_hidden(batch_size)\n",
    "            test_losses = []\n",
    "            model.eval()\n",
    "            for test_X, test_y in test_loader:\n",
    "                # test_h = tuple([e.data for e in test_h])\n",
    "                test_X = test_X.reshape(seq_len,batch_size,feature_dim).float()\n",
    "                test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "                test_out, test_h = model(test_X, test_h)\n",
    "                test_loss = criterion(test_out.squeeze(), y.float())\n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}\".format(i+1, epochs),\n",
    "                  \"Step: {}\".format(counter),\n",
    "                  \"Loss: {}\".format(loss.item()),\n",
    "                  \"Val Loss: {}\".format(np.mean(test_losses)))\n",
    "            \n",
    "            if abs(np.mean(test_losses)) <= min_loss or abs(np.mean(test_losses)-loss.item()) < min_diff:\n",
    "                print(f\"Saving {saving_index}\")\n",
    "                torch.save(model.state_dict(), f'./state_dict{saving_index}.pt')\n",
    "                min_loss = np.mean(test_losses)\n",
    "                min_diff = np.mean(test_losses)-loss.item()\n",
    "                saving_index += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"state_dict40.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.07783957570791245,\n",
       " -0.0761462152004242,\n",
       " -0.08255273848772049,\n",
       " -0.08501390367746353,\n",
       " -0.08651039749383926,\n",
       " -0.08771108835935593,\n",
       " -0.08932702243328094]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = []\n",
    "pred = []\n",
    "real = []\n",
    "index = 0\n",
    "for X, y in pred_loader:\n",
    "    i.append(index)\n",
    "    index += 1\n",
    "    X = X.reshape(seq_len,batch_size,feature_dim).float()\n",
    "    #display(X)\n",
    "    # h_tmp = tuple([e.data for e in h])\n",
    "    h_tmp = h\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    model.zero_grad()\n",
    "    output, _ = model(X, h_tmp)\n",
    "    loss = criterion(output.squeeze(), y.float())\n",
    "    display([o.item() for o in output])\n",
    "    pred += [o.item() for o in output]\n",
    "    real += [o.item() for o in y]\n",
    "#     pred.append(factor*output.item().float())\n",
    "#     real.append(factor*y.item())\n",
    "#     print(f\"prediction= {factor*output.item()}, real={factor*y.item()}, loss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.07783957570791245,\n",
       " -0.0761462152004242,\n",
       " -0.08255273848772049,\n",
       " -0.08501390367746353,\n",
       " -0.08651039749383926,\n",
       " -0.08771108835935593,\n",
       " -0.08932702243328094]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6438566666666666,\n",
       " 0.658798,\n",
       " 0.6750546666666667,\n",
       " 0.6932726666666666,\n",
       " 0.7129493333333333,\n",
       " 0.7356406666666667,\n",
       " 0.755026]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "- https://www.curiousily.com/posts/demand-prediction-with-lstms-using-tensorflow-2-and-keras-in-python/\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
